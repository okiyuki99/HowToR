---
title: "Ex MLBayesOpt"
output: html_document
---

# library

```{r}
library(rBayesianOptimization)
library(xgboost)
library(Matrix)
library(MlBayesOpt)
```

# Data Prepartion

```{r}
df <- fashion
print(table(df$y))
```

# BayesianOptiimization

```{r}
data(agaricus.train, package = "xgboost")
dtrain <- xgb.DMatrix(agaricus.train$data,
                      label = agaricus.train$label)
cv_folds <- rBayesianOptimization::KFold(agaricus.train$label, nfolds = 5,stratified = TRUE, seed = 0)
xgb_cv_bayes <- function(max_depth, min_child_weight, subsample) {
  cv <- xgb.cv(params = list(booster = "gbtree", eta = 0.01,
                             max_depth = max_depth,
                             min_child_weight = min_child_weight,
                             subsample = subsample, colsample_bytree = 0.3,
                             lambda = 1, alpha = 0,
                             objective = "binary:logistic",
                             eval_metric = "auc"),
               data = dtrain, nround = 100,
               folds = cv_folds, prediction = TRUE, showsd = TRUE,
               early_stopping_rounds = 5, maximize = TRUE, verbose = 0)
  list(Score = cv$evaluation_log$test_auc_mean[cv$best_iteration],
       Pred = cv$pred)
}
OPT_Res <- BayesianOptimization(xgb_cv_bayes,
                                bounds = list(max_depth = c(2L, 6L),
                                              min_child_weight = c(1L, 10L),
                                              subsample = c(0.5, 0.8)),
                                init_grid_dt = NULL, init_points = 10, n_iter = 20,
                                acq = "ucb", kappa = 2.576, eps = 0.0,
                                verbose = TRUE)
```

# MLBayesOpt

```{r}
res0 <- xgb_cv_opt(data = df,
                   label = y,
                   objectfun = "multi:softmax",
                   evalmetric = "merror",
                   n_folds = 15,
                   classes = 10)
str(res0)

print(res0$Best_Par)
```

